import time
import json
import pandas as pd
from rag_system import RAGSystem
from tqdm import tqdm

# åŠ è½½æµ‹è¯•é›†ï¼ˆè¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨å®šä¹‰å°‘é‡ Ground Truth ç”¨äºæ¼”ç¤ºï¼Œå®é™…ä½œä¸šå¯ä»¥ä» medical.json æŠ½ 50 æ¡ï¼‰
TEST_DATA = [
    {
        "question": "æ„Ÿå†’äº†å—“å­ç–¼æ€ä¹ˆåŠï¼Ÿ",
        "ground_truth": "å»ºè®®å¤šå–æ¸©æ°´ï¼Œæœç”¨è“èŠ©å£æœæ¶²ã€‚é¥®é£Ÿæ¸…æ·¡ï¼Œå¿Œè¾›è¾£ã€‚",
    },
    {
        "question": "ç³–å°¿ç—…é¥®é£Ÿç¦å¿Œï¼Ÿ",
        "ground_truth": "æ§åˆ¶ç³–åˆ†ï¼Œå°‘åƒç”œé£Ÿï¼Œä¸»é£Ÿå®šé‡ï¼Œå¤šåƒç²—ç²®è”¬èœã€‚",
    },
    {
        "question": "é«˜è¡€å‹èƒ½å½»åº•æ²»æ„ˆå—ï¼Ÿ",
        "ground_truth": "åŸå‘æ€§é«˜è¡€å‹ç›®å‰æ— æ³•å½»åº•æ²»æ„ˆï¼Œéœ€è¦ç»ˆèº«æœè¯æ§åˆ¶ã€‚",
    }
]

def evaluate_system():
    print("æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿè¿›è¡Œè¯„ä¼°...")
    rag = RAGSystem()

    # =========== ä¿®æ”¹ç‚¹å¼€å§‹ ===========
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    # 1. ç”¨å˜é‡ docs æ¥æ”¶è¿”å›çš„æ–‡æ¡£åˆ—è¡¨
    docs = rag.load_medical_data("data/medical.json")

    if not docs:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ data/medical.json æ˜¯å¦å­˜åœ¨")
        return

    print(f"æˆåŠŸåŠ è½½ {len(docs)} æ¡æ•°æ®ï¼Œæ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
    # 2. å°† docs ä¼ å…¥æ„å»ºå‡½æ•°
    rag.build_vectorstore(docs)
    # =========== ä¿®æ”¹ç‚¹ç»“æŸ ===========

    rag.init_qa_chain()

    results = []

    print(f"\nå¼€å§‹è¯„ä¼° {len(TEST_DATA)} æ¡æµ‹è¯•æ•°æ®...")
    for item in tqdm(TEST_DATA):
        q = item["question"]
        gt = item["ground_truth"]

        # è·å– RAG å›ç­”
        response = rag.ask_question(q)
        pred = response["answer"]

        # åˆ¤æ–­æ˜¯å¦åŒ…å«å¼•ç”¨
        has_citation = "[ç‰‡æ®µ" in pred or "åŸºäºçŸ¥è¯†åº“" in pred or "[åŸºäº" in pred

        # --- LLM-as-a-Judge: è®©å¤§æ¨¡å‹ç»™è¿™ä¸ªå›ç­”æ‰“åˆ† ---
        eval_prompt = f"""
        è¯·ä½œä¸ºä¸€åå…¬æ­£çš„è¯„åˆ¤è€…ï¼Œå¯¹æ¯”â€œæ ‡å‡†ç­”æ¡ˆâ€å’Œâ€œç³»ç»Ÿå›ç­”â€ã€‚
        
        é—®é¢˜ï¼š{q}
        æ ‡å‡†ç­”æ¡ˆï¼š{gt}
        ç³»ç»Ÿå›ç­”ï¼š{pred}
        
        è¯·æ‰“åˆ†ï¼ˆ0-10åˆ†ï¼‰ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦åŒ…å«å¹»è§‰ï¼ˆæ˜¯/å¦ï¼‰ã€‚
        åªè¾“å‡ºJSONæ ¼å¼ï¼Œä¾‹å¦‚ï¼š{{"score": 8, "hallucination": "å¦"}}
        """

        score = 0
        is_hallucination = "æœªçŸ¥"

        try:
            # è°ƒç”¨ LLM è¿›è¡Œè¯„åˆ†
            # æ³¨æ„ï¼šå¦‚æœ DeepSeek API ä¸ç¨³å®šï¼Œè¿™ä¸€æ­¥å¯èƒ½ä¼šæ…¢æˆ–å¤±è´¥
            eval_res_str = rag.llm._call(eval_prompt)
            # æ¸…ç†å¯èƒ½çš„ markdown æ ‡è®°
            eval_res_str = eval_res_str.replace("```json", "").replace("```", "").strip()
            eval_res = json.loads(eval_res_str)

            score = eval_res.get("score", 0)
            is_hallucination = eval_res.get("hallucination", "å¦")
        except Exception as e:
            print(f"è¯„åˆ†å¤±è´¥: {e}")
            score = 5 # è§£æå¤±è´¥ç»™ä¿åº•åˆ†

        results.append({
            "question": q,
            "answer": pred,
            "score": score,
            "has_citation": has_citation,
            "hallucination": is_hallucination
        })

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    if not results:
        print("æ²¡æœ‰è¯„ä¼°ç»“æœ")
        return

    df = pd.DataFrame(results)
    avg_score = df["score"].mean()
    citation_rate = (df["has_citation"].sum() / len(df)) * 100

    # ç®€å•çš„å¹»è§‰ç‡è®¡ç®—
    hallucination_count = len(df[df["hallucination"] == "æ˜¯"])
    hallucination_rate = (hallucination_count / len(df)) * 100

    print("\n" + "="*40)
    print("ğŸ“Š è¯„ä¼°æŠ¥å‘Š (Evaluation Report)")
    print("="*40)
    print(f"âœ… å¹³å‡å‡†ç¡®å¾—åˆ†: {avg_score:.2f} / 10.0")
    print(f"ğŸ“š å¼•ç”¨è¦†ç›–ç‡:   {citation_rate:.2f}%")
    print(f"âš ï¸ å¹»è§‰ç‡:       {hallucination_rate:.2f}%")
    print("="*40)

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    output_file = "evaluation_report.csv"
    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³ {output_file}")

if __name__ == "__main__":
    evaluate_system()